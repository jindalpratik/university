{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# libraries used for graph and visulization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "\n",
    "# libraries used for missing value \n",
    "import missingno as msno \n",
    "\n",
    "#libraries used for preprocessing and modeling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, roc_curve, f1_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Lending_Club_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df =df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(loan_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "msno.bar(loan_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loan_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.emp_title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.emp_title.value_counts().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.emp_title.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {'A': [1, 2, 3], 'B': [4, 5, 6]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop column 'B'\n",
    "df.drop(columns=['B'], inplace=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with missing values\n",
    "data = {'A': [1, 2, None], 'B': [4, None, 6]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop rows with any NaN values\n",
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "data = {'A': [1, 2, None], 'B': [4, None, 6], 'C': [None, 8, 9]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Drop columns with any NaN values\n",
    "df.dropna(axis=1, inplace=True)\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(\"\\nDataFrame after dropping columns with NaN values:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = pd.DataFrame(np.arange(25).reshape(5, 5),  \n",
    "                      index=list('abcde'), \n",
    "                      columns=['x','y','z', 'a', 'b'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['c': 'd' , :'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data won’t be meaningful and any relationship we might observe might be due to confounding relationships\n",
    "#A more advanced implementation might look to group all these job descriptions into categories and/or examine\n",
    "#if Lending Club’s model looks at (annual_inc + emp_title) versus just annual_inc\n",
    "loan_df.drop(['emp_title'],1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping columns having misleading values, missing values > 50%, indentity values, encrypted values\n",
    "loan_df.drop(['mths_since_last_delinq','mths_since_last_record','collections_12_mths_ex_med','Notes','purpose','earliest_cr_line','Id','collections_12_mths_ex_med','initial_list_status','zip_code'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loan_df.emp_length.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling na values with avreage values\n",
    "loan_df['emp_length'] = pd.to_numeric(loan_df['emp_length'], errors='coerce')\n",
    "avg_value=loan_df['emp_length'].median()\n",
    "loan_df['emp_length'] =loan_df['emp_length'].fillna(avg_value)\n",
    "loan_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERIFIED - income and VERIFIED - income source could potentially be the same criteria\n",
    "loan_df.verification_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in loan_df._get_numeric_data().columns:\n",
    "    loan_df[i] = loan_df[i].fillna(loan_df[i].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3],[4,5,6],[7,8,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_upper_diag = np.triu(a, k=0)\n",
    "tri_upper_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_lower_diag = np.tril(a, k=0)\n",
    "tri_lower_diag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA - on Leanding club data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = loan_df.corr()\n",
    "cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#highly corelated attributes\n",
    "cor = loan_df.corr()\n",
    "#cor.loc[:,:] = np.tril(cor, k=-1) \n",
    "cor = cor.stack()\n",
    "cor[(cor > 0.55) | (cor < -0.55)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.drop(['total_acc'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.pymnt_plan.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.pymnt_plan.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping pymnt_plan, it is identity attribute\n",
    "loan_df.drop(['pymnt_plan'],1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loan_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation Matrix\n",
    "loan_cor = loan_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heat map for correlation\n",
    "mask = np.zeros_like(loan_cor, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 9))\n",
    "\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "sns.heatmap(loan_cor, mask=mask,\n",
    "            vmax=.3,\n",
    "            cmap=cmap,\n",
    "            square=True,\n",
    "            linewidths=.5,\n",
    "            cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysing loan purpose\n",
    "purpose = loan_df.purpose_cat.value_counts()\n",
    "purpose.plot.barh(figsize =(10,5),color =\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist =sns.countplot(data=loan_df,hue='is_bad',x='purpose_cat')\n",
    "dist.set_xticklabels(dist.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.title('Loan distribution')\n",
    "plt.xlabel('purpose',)\n",
    "plt.ylabel('status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist =sns.countplot(data=loan_df,hue='is_bad',x='verification_status')\n",
    "dist.set_xticklabels(dist.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.title('Loan distribution')\n",
    "plt.xlabel('purpose',)\n",
    "plt.ylabel('status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysing bad and not bad loan\n",
    "is_bad_loan = loan_df[loan_df.is_bad == 1]\n",
    "not_bad_loan = loan_df[loan_df.is_bad == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(not_bad_data.shape)\n",
    "print(is_bad_data.shape)\n",
    "print(loan_df['is_bad'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_bad_loan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x= is_bad_loan['annual_inc'], y= is_bad_data['debt_to_income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x= not_bad_data['annual_inc'], y= not_bad_data['debt_to_income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(loan_df, hue=\"is_bad\",size=5) \\\n",
    "   .map(plt.scatter, \"annual_inc\", \"debt_to_income\") \\\n",
    "   .add_legend(title = 'Staus', labels = ['Approved','Rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.home_ownership.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.addr_state.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.policy_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping policy_code, not relevent for investigation\n",
    "loan_df.drop(['policy_code'],1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping addr_state, not relevent for investigation\n",
    "loan_df.drop(['addr_state'],1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with categorical attributes\n",
    "\n",
    "# Create dummy variables from the feature purpose_cat\n",
    "loan_df = pd.get_dummies(loan_df, columns=[\"purpose_cat\"], drop_first=True)\n",
    "\n",
    "# Create dummy variables from the feature home_ownership \n",
    "loan_df = pd.get_dummies(loan_df, columns=[\"home_ownership\"], drop_first=True)\n",
    "\n",
    "# Create dummy variables from the feature verification_status\n",
    "loan_df = pd.get_dummies(loan_df, columns=[\"verification_status\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = list(loan_df)\n",
    "Independent = my_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = loan_df[Independent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depended = ['is_bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = loan_df[depended]\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randon Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train[num]= sc.fit_transform(X_train[num])\n",
    "X_test[num] = sc.transform(X_test[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(criterion = 'entropy',\n",
    "                                                   max_depth= 4,\n",
    "                                                   max_features ='log2',\n",
    "                                                   n_estimators = 100,\n",
    "                                                   min_samples_split = 6,\n",
    "                                                   class_weight = {0: 1, 1: 5}\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_rf= rf_classifier.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision', precision_score(y_train, y_pred_train_rf))\n",
    "print('Accuracy', accuracy_score(y_train, y_pred_train_rf))\n",
    "print('F1 Score', f1_score(y_train, y_pred_train_rf))\n",
    "print('Recall', recall_score(y_train, y_pred_train_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_rf = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision', precision_score(y_test, y_pred_test_rf))\n",
    "print('Accuracy', accuracy_score(y_test, y_pred_test_rf))\n",
    "print('F1 Score', f1_score(y_test, y_pred_test_rf))\n",
    "print('Recall', recall_score(y_test, y_pred_test_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_rf = confusion_matrix(y_test,y_pred_test)\n",
    "print(cm_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC(probability = True)\n",
    "svm_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_svm= svm_classifier.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision', precision_score(y_train, y_pred_train_svm))\n",
    "print('Accuracy', accuracy_score(y_train, y_pred_train_svm))\n",
    "print('F1 Score', f1_score(y_train, y_pred_train_svm))\n",
    "print('Recall', recall_score(y_train, y_pred_train_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_svm = svm_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision', precision_score(y_test, y_pred_test_svm))\n",
    "print('Accuracy', accuracy_score(y_test, y_pred_test_svm))\n",
    "print('F1 Score', f1_score(y_test, y_pred_test_svm))\n",
    "print('Recall', recall_score(y_test, y_pred_test_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_svm = confusion_matrix(y_test,y_pred_test_svm)\n",
    "print(cm_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "feature_importances = pd.DataFrame(rf_classifier.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance',     \n",
    "                                                                        ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=feature_importances[:10].importance, y=feature_importances[:10].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_sm= svm_classifier.predict_proba(X_test.values)\n",
    "y_scores_rf = rf_classifier.predict_proba(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_rf, tpr_rf, thresholds_rf= roc_curve(y_test, y_scores_rf[:,1])\n",
    "fpr_svm, tpr_svm, thresholds_svm= roc_curve(y_test, y_scores_sm[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.plot(fpr_rf, tpr_rf, label= 'Random Forest')\n",
    "plt.plot(fpr_svm, tpr_svm, label= 'SVM')\n",
    "\n",
    "plt.plot([0,1], [0,1], linestyle='--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_sm_train = svm_classifier.predict_proba(X_train.values)\n",
    "y_scores_rf_train = rf_classifier.predict_proba(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_rf_train, tpr_rf_train, thresholds_rf_train= roc_curve(y_train, y_scores_rf_train[:,1])\n",
    "fpr_svm_train, tpr_svm_train, thresholds_svm_train= roc_curve(y_train, y_scores_sm_train[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.plot(fpr_rf_train, tpr_rf_train, label= 'Random Forest')\n",
    "plt.plot(fpr_svm_train, tpr_svm_train, label= 'SVM')\n",
    "\n",
    "plt.plot([0,1], [0,1], linestyle='--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dict_objects = {'Standard Scaling':sc, \n",
    "                'Random Forest Classifier': rf_classifier,\n",
    "                'SVM Classifier': svm_classifier\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ModelPickles.pkl'\n",
    "outfile = open(filename,'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dict_objects,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "def predict_json(project, model, instances, version=None):\n",
    "    \"\"\"Send json data to a deployed model for prediction.\n",
    "    Args:\n",
    "        project (str): project where the AI Platform Prediction Model is deployed.\n",
    "        model (str): model name.\n",
    "        instances ([[float]]): List of input instances, where each input\n",
    "           instance is a list of floats.\n",
    "        version: str, version of the model to target.\n",
    "    Returns:\n",
    "        Mapping[str: any]: dictionary of prediction results defined by the\n",
    "            model.\n",
    "    \"\"\"\n",
    "    # Create the AI Platform Prediction service object.\n",
    "    # To authenticate set the environment variable\n",
    "    # GOOGLE_APPLICATION_CREDENTIALS=<path_to_service_account_file>\n",
    "    service = googleapiclient.discovery.build('ml', 'v1')\n",
    "    name = 'projects/{}/models/{}'.format(project, model)\n",
    "\n",
    "    if version is not None:\n",
    "        name += '/versions/{}'.format(version)\n",
    "\n",
    "    response = service.projects().predict(\n",
    "        name=name,\n",
    "        body={'instances': instances}\n",
    "    ).execute()\n",
    "   \n",
    "    if 'error' in response:\n",
    "        raise RuntimeError(response['error'])\n",
    "\n",
    "    return response['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'xxxxxxxxxxxxx.json'\n",
    "\n",
    "test = X_test.iloc[20:21].values\n",
    "test.shape\n",
    "test.reshape(1, -1)\n",
    "instances = test.tolist()\n",
    "predict_json('mwpmltr', 'lending_club_model', instances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
